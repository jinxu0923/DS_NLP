{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265134f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcf0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def read_text(self,is_train_data):\n",
    "        #read that data\n",
    "        #is_train_data==True   reading the training data \n",
    "        #is_train_data==False  reading the testing data \n",
    "        datas = []\n",
    "        labels = []\n",
    "        if(is_train_data):\n",
    "            #training data folder\n",
    "            INFOCOM_path = \"./data/train/INFOCOM/\"\n",
    "            ISCAS_path = \"./data/train/ISCAS/\"\n",
    "            SIGGRAPH_path = \"./data/train/SIGGRAPH/\"\n",
    "            VLDB_path = \"./data/train/VLDB/\"\n",
    "            WWW_path = \"./data/train/WWW/\" \n",
    "        else:\n",
    "            #testing data folder\n",
    "            INFOCOM_path = \"./data/test/INFOCOM/\"\n",
    "            ISCAS_path = \"./data/test/ISCAS/\"\n",
    "            SIGGRAPH_path = \"./data/test/SIGGRAPH/\"\n",
    "            VLDB_path = \"./data/test/VLDB/\"\n",
    "            WWW_path = \"./data/test/WWW/\"\n",
    "        INFOCOM_files= os.listdir(INFOCOM_path) \n",
    "        ISCAS_files = os.listdir(ISCAS_path)\n",
    "        SIGGRAPH_files = os.listdir(SIGGRAPH_path)\n",
    "        VLDB_files = os.listdir(VLDB_path)\n",
    "        WWW_files = os.listdir(WWW_path)\n",
    "        \n",
    "        \n",
    "        for file_name in INFOCOM_files: \n",
    "            file_position = INFOCOM_path + file_name\n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read() \n",
    "                datas.append(data)\n",
    "                labels.append([1,0,0,0,0])  #transform the label into vector\n",
    "        \n",
    "        for file_name in ISCAS_files:\n",
    "            file_position = ISCAS_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,1,0,0,0])  #transform the label into vector\n",
    "\n",
    "        for file_name in SIGGRAPH_files:\n",
    "            file_position = SIGGRAPH_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,1,0,0])  #transform the label into vector\n",
    "\n",
    "        for file_name in VLDB_files:\n",
    "            file_position = VLDB_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,0,1,0])  #transform the label into vector\n",
    "\n",
    "        for file_name in WWW_files:\n",
    "            file_position = WWW_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,0,0,1])  #transform the label into vector\n",
    "        return datas, labels\n",
    "    \n",
    "    def word_count(self, datas):\n",
    "        \n",
    "        dic = {}\n",
    "        for data in datas:\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower()\n",
    "                if(word in dic):\n",
    "                    dic[word] += 1\n",
    "                else:\n",
    "                    dic[word] = 1\n",
    "        word_count_sorted = sorted(dic.items(), key=lambda item:item[1], reverse=True)\n",
    "        return  word_count_sorted\n",
    "    \n",
    "    def word_index(self, datas, vocab_size):\n",
    "       \n",
    "        word_count_sorted = self.word_count(datas)\n",
    "        word2index = {}\n",
    "        \n",
    "        word2index[\"<unk>\"] = 0\n",
    "        \n",
    "        word2index[\"<pad>\"] = 1\n",
    "        \n",
    "        \n",
    "        vocab_size = min(len(word_count_sorted), vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            word = word_count_sorted[i][0]\n",
    "            word2index[word] = i + 2\n",
    "          \n",
    "        return word2index, vocab_size\n",
    "    \n",
    "    def get_datasets(self, vocab_size, embedding_size, max_len):\n",
    "        \n",
    "        train_datas, train_labels = self.read_text(is_train_data=True)\n",
    "        word2index, vocab_size = self.word_index(train_datas, vocab_size)\n",
    "        \n",
    "        test_datas, test_labels = self.read_text(is_train_data = False)\n",
    "        \n",
    "        train_features = []\n",
    "        for data in train_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower()\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"])\n",
    "                if(len(feature)==max_len):\n",
    "                    break\n",
    "            \n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            train_features.append(feature)\n",
    "            \n",
    "        test_features = []\n",
    "        for data in test_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() \n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"]) \n",
    "                if(len(feature)==max_len):\n",
    "                    break\n",
    "           \n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            test_features.append(feature)\n",
    "            \n",
    "        train_features = torch.LongTensor(train_features)\n",
    "        train_labels = torch.FloatTensor(train_labels)\n",
    "        \n",
    "        test_features = torch.LongTensor(test_features)\n",
    "        test_labels = torch.FloatTensor(test_labels)\n",
    "        \n",
    "        embed = nn.Embedding(vocab_size + 2, embedding_size)\n",
    "        train_features = embed(train_features)\n",
    "        test_features = embed(test_features)\n",
    "        \n",
    "        train_features = Variable(train_features, requires_grad=False)\n",
    "        train_datasets = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "        \n",
    "        test_features = Variable(test_features, requires_grad=False)\n",
    "        test_datasets = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "        return train_datasets, test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8437d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) \n",
    "\n",
    "vocab_size = 31000   #size of vocabulary\n",
    "embedding_size = 100  #size of word embedding\n",
    "num_classes = 5    #5 classification\n",
    "sentence_max_len = 20  #the maximum of sentence length\n",
    "hidden_size = 16\n",
    "\n",
    "num_layers = 1 #one layer of lstm\n",
    "num_directions = 2  #bidirectional lstm\n",
    "lr = 1e-3\n",
    "batch_size = 16   \n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66df9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "190331bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-LSTM model\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size,hidden_size, num_layers, num_directions, num_classes):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        \n",
    "        self.input_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        self.attention_weights_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.liner = nn.Linear(hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #lstm's shape [seq_len, batch, input_size]\n",
    "        #x [batch_size, sentence_length, embedding_size]\n",
    "        x = x.permute(1, 0, 2)         #[sentence_length, batch_size, embedding_size]\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        \n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        #out[seq_len, batch, num_directions * hidden_size]\n",
    "        #h_n, c_n [num_layers * num_directions, batch, hidden_size]\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        #print(out.shape) #20, 16, 32\n",
    "        \n",
    "        #split the output of lstm into forward and backward\n",
    "        (forward_out, backward_out) = torch.chunk(out, 2, dim = 2)\n",
    "        out = forward_out + backward_out  #[seq_len, batch, hidden_size]\n",
    "        out = out.permute(1, 0, 2)  #[batch, seq_len, hidden_size] #16,20,16\n",
    "        \n",
    "        h_n = h_n.permute(1, 0, 2)  #[batch, num_layers * num_directions,  hidden_size]\n",
    "        h_n = torch.sum(h_n, dim=1) #[batch, 1,  hidden_size]\n",
    "        h_n = h_n.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        \n",
    "        attention_w = self.attention_weights_layer(h_n)  #[batch, hidden_size]\n",
    "        attention_w = attention_w.unsqueeze(dim=1) #[batch, 1, hidden_size]\n",
    "        \n",
    "        #print(out.transpose(1,2).shape) #16, 16, 20\n",
    "        attention_context = torch.bmm(attention_w, out.transpose(1, 2))  #[batch, 1, seq_len]\n",
    "        softmax_w = F.softmax(attention_context, dim=-1)  #[batch, 1, seq_len]\n",
    "        \n",
    "        x = torch.bmm(softmax_w, out)  #[batch, 1, hidden_size]\n",
    "        x = x.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        x = self.liner(x)\n",
    "        x = self.act_func(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947c6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    preds_total = []\n",
    "    labels_total = []\n",
    "    for datas, labels in test_loader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(datas)\n",
    "        loss = loss_func(preds, labels)\n",
    "        \n",
    "        loss_val += loss.item() * datas.size(0)\n",
    "        \n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        preds_total.extend(preds.cpu().numpy().tolist())\n",
    "        labels_total.extend(labels.cpu().numpy().tolist())\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader.dataset)\n",
    "    test_acc = corrects / len(test_loader.dataset)\n",
    "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\n",
    "    print(\"precision \", precision_score(labels_total, preds_total, average='macro'))\n",
    "    print(\"recall \", recall_score(labels_total, preds_total, average='macro'))\n",
    "    print(\"f1 \", f1_score(labels_total, preds_total, average='macro'))\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f8e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader,test_loader, optimizer, loss_func, epochs):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch: \", epoch)\n",
    "        model.train()\n",
    "        loss_val = 0.0\n",
    "        corrects = 0.0\n",
    "        for datas, labels in train_loader:\n",
    "            datas = datas.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(datas)\n",
    "            loss = loss_func(preds, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_val += loss.item() * datas.size(0)\n",
    "            \n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            corrects += torch.sum(preds == labels).item()\n",
    "        train_loss = loss_val / len(train_loader.dataset)\n",
    "        train_acc = corrects / len(train_loader.dataset)\n",
    "        if(epoch % 1 == 0):\n",
    "            print(\"Train Loss: {}, Train Acc: {}\".format(train_loss, train_acc))\n",
    "            test_acc = test(model, test_loader, loss_func)\n",
    "            if(best_val_acc < test_acc):\n",
    "                best_val_acc = test_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed94221",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor()\n",
    "train_datasets, test_datasets = processor.get_datasets(vocab_size=vocab_size, embedding_size=embedding_size, max_len=sentence_max_len)\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce170b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444ceae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMModel(embedding_size, hidden_size, num_layers, num_directions, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a23bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f375a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMModel(\n",
       "  (lstm): LSTM(100, 16, bidirectional=True)\n",
       "  (attention_weights_layer): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (liner): Linear(in_features=16, out_features=5, bias=True)\n",
       "  (act_func): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff44580",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d14fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51013f45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Train Loss: 0.40020485886120505, Train Acc: 0.5081088573672781\n",
      "Test Loss: 0.34175438917853024, Test Acc: 0.6134005336495701\n",
      "precision  0.45989838756181484\n",
      "recall  0.48919702794727443\n",
      "f1  0.45761727549658876\n",
      "epoch:  1\n",
      "Train Loss: 0.2965254820967615, Train Acc: 0.6703784133438063\n",
      "Test Loss: 0.2964874910930127, Test Acc: 0.6560924992588201\n",
      "precision  0.49652106783364014\n",
      "recall  0.5297756216289002\n",
      "f1  0.49761920150994665\n",
      "epoch:  2\n",
      "Train Loss: 0.2544723475827079, Train Acc: 0.7214803862680774\n",
      "Test Loss: 0.2751138279037238, Test Acc: 0.7067892084198043\n",
      "precision  0.559461802272086\n",
      "recall  0.5618126067014975\n",
      "f1  0.5468746681293674\n",
      "epoch:  3\n",
      "Train Loss: 0.22767963450403225, Train Acc: 0.7542854502610544\n",
      "Test Loss: 0.27819293046912985, Test Acc: 0.6943373851171064\n",
      "precision  0.5571110453811385\n",
      "recall  0.5828766322085662\n",
      "f1  0.5512504629530308\n",
      "epoch:  4\n",
      "Train Loss: 0.2078403016737449, Train Acc: 0.7782192856812826\n",
      "Test Loss: 0.2772442317401271, Test Acc: 0.7088645123035873\n",
      "precision  0.5590236333230632\n",
      "recall  0.5740218575300252\n",
      "f1  0.5542022357848595\n",
      "epoch:  5\n",
      "Train Loss: 0.19407461430457978, Train Acc: 0.7923116019036178\n",
      "Test Loss: 0.2859421916507028, Test Acc: 0.7002668247850579\n",
      "precision  0.5591322824759921\n",
      "recall  0.5975019291568259\n",
      "f1  0.5622268909500578\n",
      "epoch:  6\n",
      "Train Loss: 0.1833119275618882, Train Acc: 0.8056646490782239\n",
      "Test Loss: 0.28187861383014284, Test Acc: 0.7162763118885266\n",
      "precision  0.5766309717390735\n",
      "recall  0.6126614251083151\n",
      "f1  0.5794826887382415\n",
      "epoch:  7\n",
      "Train Loss: 0.17342050417480398, Train Acc: 0.8164302545857783\n",
      "Test Loss: 0.2911481280868286, Test Acc: 0.7156833679217314\n",
      "precision  0.5709774446637372\n",
      "recall  0.599205007305252\n",
      "f1  0.5720887255412046\n",
      "epoch:  8\n",
      "Train Loss: 0.1638798519899448, Train Acc: 0.8284895809268585\n",
      "Test Loss: 0.30717477546118505, Test Acc: 0.6928550252001185\n",
      "precision  0.5643115468965175\n",
      "recall  0.6226825641916423\n",
      "f1  0.56839016319766\n",
      "epoch:  9\n",
      "Train Loss: 0.15774030606168363, Train Acc: 0.8356050455112507\n",
      "Test Loss: 0.31169330054579875, Test Acc: 0.7198339756892974\n",
      "precision  0.5772348280858806\n",
      "recall  0.5990664181930156\n",
      "f1  0.5739201364087159\n",
      "epoch:  10\n",
      "Train Loss: 0.15021268283988837, Train Acc: 0.8449845215543131\n",
      "Test Loss: 0.310585905846099, Test Acc: 0.7213163356062852\n",
      "precision  0.58425360103535\n",
      "recall  0.6250087342611135\n",
      "f1  0.5910107634949004\n",
      "epoch:  11\n",
      "Train Loss: 0.14323654994991644, Train Acc: 0.852608233609019\n",
      "Test Loss: 0.3140984399759412, Test Acc: 0.733175214942188\n",
      "precision  0.5956684189046234\n",
      "recall  0.6161755109127842\n",
      "f1  0.5942253800642058\n",
      "epoch:  12\n",
      "Train Loss: 0.138663748435589, Train Acc: 0.8569514392644273\n",
      "Test Loss: 0.32897051360669083, Test Acc: 0.7340646308923807\n",
      "precision  0.5926327847714827\n",
      "recall  0.6123160850700708\n",
      "f1  0.5948707554293348\n",
      "epoch:  13\n",
      "Train Loss: 0.1351192409805814, Train Acc: 0.8625883657533614\n",
      "Test Loss: 0.3425703628318373, Test Acc: 0.731099911058405\n",
      "precision  0.5912149980716126\n",
      "recall  0.6111087782608007\n",
      "f1  0.5909639855650008\n",
      "epoch:  14\n",
      "Train Loss: 0.12700338673203027, Train Acc: 0.8711823684332116\n",
      "Test Loss: 0.34350385481760615, Test Acc: 0.7192410317225022\n",
      "precision  0.5734663848511341\n",
      "recall  0.6168463077547311\n",
      "f1  0.5831120560226447\n",
      "epoch:  15\n",
      "Train Loss: 0.12543690723988793, Train Acc: 0.8739546273621956\n",
      "Test Loss: 0.3503818566036479, Test Acc: 0.7263563593240439\n",
      "precision  0.5803561800327541\n",
      "recall  0.610040593478566\n",
      "f1  0.5869381894567394\n",
      "epoch:  16\n",
      "Train Loss: 0.12236133699071654, Train Acc: 0.8771889294460102\n",
      "Test Loss: 0.3546267636639408, Test Acc: 0.7100504002371776\n",
      "precision  0.5683770024815687\n",
      "recall  0.62428776249653\n",
      "f1  0.5811480176060172\n",
      "epoch:  17\n",
      "Train Loss: 0.11746515233820293, Train Acc: 0.8835189206671903\n",
      "Test Loss: 0.3866032264086319, Test Acc: 0.7396975985769345\n",
      "precision  0.5998817565735972\n",
      "recall  0.598761025333195\n",
      "f1  0.5922406573372221\n",
      "epoch:  18\n",
      "Train Loss: 0.11508181440831883, Train Acc: 0.8855519105484452\n",
      "Test Loss: 0.38813146066142584, Test Acc: 0.7352505188259709\n",
      "precision  0.5927592299873514\n",
      "recall  0.6088711623467409\n",
      "f1  0.5933179242689117\n",
      "epoch:  19\n",
      "Train Loss: 0.11240808892061688, Train Acc: 0.8899413205193365\n",
      "Test Loss: 0.3935217373349212, Test Acc: 0.7260598873406463\n",
      "precision  0.579561782789482\n",
      "recall  0.6130656009403445\n",
      "f1  0.5876890895778611\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, test_loader, optimizer, loss_func, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d76342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.385274834213008, Test Acc: 0.7388081826267417\n",
      "precision  0.6025217308595123\n",
      "recall  0.5969096195935283\n",
      "f1  0.5926871426375425\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model, test_loader, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "312fd4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7388081826267417"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb8782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
