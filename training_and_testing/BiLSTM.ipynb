{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6817ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcf0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def read_text(self,is_train_data):\n",
    "        #read that data\n",
    "        #is_train_data==True   reading the training data \n",
    "        #is_train_data==False  reading the testing data \n",
    "        datas = []\n",
    "        labels = []\n",
    "        if(is_train_data):\n",
    "            #training data folder\n",
    "            INFOCOM_path = \"./data/train/INFOCOM/\"\n",
    "            ISCAS_path = \"./data/train/ISCAS/\"\n",
    "            SIGGRAPH_path = \"./data/train/SIGGRAPH/\"\n",
    "            VLDB_path = \"./data/train/VLDB/\"\n",
    "            WWW_path = \"./data/train/WWW/\" \n",
    "        else:\n",
    "            #testing data folder\n",
    "            INFOCOM_path = \"./data/test/INFOCOM/\"\n",
    "            ISCAS_path = \"./data/test/ISCAS/\"\n",
    "            SIGGRAPH_path = \"./data/test/SIGGRAPH/\"\n",
    "            VLDB_path = \"./data/test/VLDB/\"\n",
    "            WWW_path = \"./data/test/WWW/\"\n",
    "        INFOCOM_files= os.listdir(INFOCOM_path)\n",
    "        ISCAS_files = os.listdir(ISCAS_path)\n",
    "        SIGGRAPH_files = os.listdir(SIGGRAPH_path)\n",
    "        VLDB_files = os.listdir(VLDB_path)\n",
    "        WWW_files = os.listdir(WWW_path)\n",
    "        \n",
    "        \n",
    "        for file_name in INFOCOM_files: \n",
    "            file_position = INFOCOM_path + file_name\n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:  \n",
    "                data = f.read()   \n",
    "                datas.append(data)\n",
    "                labels.append([1,0,0,0,0]) #transform the label into vector\n",
    "        \n",
    "        for file_name in ISCAS_files:\n",
    "            file_position = ISCAS_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,1,0,0,0]) #transform the label into vector\n",
    "\n",
    "        for file_name in SIGGRAPH_files:\n",
    "            file_position = SIGGRAPH_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,1,0,0]) #transform the label into vector\n",
    "\n",
    "        for file_name in VLDB_files:\n",
    "            file_position = VLDB_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,0,1,0]) #transform the label into vector\n",
    "\n",
    "        for file_name in WWW_files:\n",
    "            file_position = WWW_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,0,0,1]) #transform the label into vector\n",
    "        return datas, labels\n",
    "    \n",
    "    def word_count(self, datas):\n",
    "        dic = {}\n",
    "        for data in datas:\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower()\n",
    "                if(word in dic):\n",
    "                    dic[word] += 1\n",
    "                else:\n",
    "                    dic[word] = 1\n",
    "        word_count_sorted = sorted(dic.items(), key=lambda item:item[1], reverse=True)\n",
    "        return  word_count_sorted\n",
    "    \n",
    "    def word_index(self, datas, vocab_size):\n",
    "        word_count_sorted = self.word_count(datas)\n",
    "        word2index = {}\n",
    "        word2index[\"<unk>\"] = 0\n",
    "        word2index[\"<pad>\"] = 1\n",
    "        \n",
    "        vocab_size = min(len(word_count_sorted), vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            word = word_count_sorted[i][0]\n",
    "            word2index[word] = i + 2\n",
    "          \n",
    "        return word2index, vocab_size\n",
    "    \n",
    "    def get_datasets(self, vocab_size, embedding_size, max_len):\n",
    "        train_datas, train_labels = self.read_text(is_train_data=True)\n",
    "        word2index, vocab_size = self.word_index(train_datas, vocab_size)\n",
    "        \n",
    "        test_datas, test_labels = self.read_text(is_train_data = False)\n",
    "        \n",
    "        train_features = []\n",
    "        for data in train_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() \n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"]) \n",
    "                if(len(feature)==max_len): \n",
    "                    break\n",
    "\n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            train_features.append(feature)\n",
    "            \n",
    "        test_features = []\n",
    "        for data in test_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower()\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"])\n",
    "                if(len(feature)==max_len):\n",
    "                    break\n",
    "            \n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            test_features.append(feature)\n",
    "            \n",
    "        train_features = torch.LongTensor(train_features)\n",
    "        train_labels = torch.FloatTensor(train_labels)\n",
    "        \n",
    "        test_features = torch.LongTensor(test_features)\n",
    "        test_labels = torch.FloatTensor(test_labels)\n",
    "        \n",
    "        embed = nn.Embedding(vocab_size + 2, embedding_size)\n",
    "        train_features = embed(train_features)\n",
    "        test_features = embed(test_features)\n",
    "        \n",
    "        train_features = Variable(train_features, requires_grad=False)\n",
    "        train_datasets = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "        \n",
    "        test_features = Variable(test_features, requires_grad=False)\n",
    "        test_datasets = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "        return train_datasets, test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8437d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "vocab_size = 31000   #size of vocabulary\n",
    "embedding_size = 100   #size of word embedding\n",
    "num_classes = 5    #5 classification\n",
    "sentence_max_len = 20  #the maximum of sentence length\n",
    "hidden_size = 16\n",
    "\n",
    "num_layers = 1  #one layer of lstm\n",
    "num_directions = 2  #bidirectional lstm\n",
    "lr = 1e-3\n",
    "batch_size = 32   \n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66df9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "190331bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-LSTM model\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size,hidden_size, num_layers, num_directions, num_classes):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        \n",
    "        self.input_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        #self.attention_weights_layer = nn.Sequential(\n",
    "         #   nn.Linear(hidden_size, hidden_size),\n",
    "         #   nn.ReLU(inplace=True)\n",
    "        #)\n",
    "        self.liner = nn.Linear(hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #lstm's shape [seq_len, batch, input_size]\n",
    "        #x [batch_size, sentence_length, embedding_size]\n",
    "        x = x.permute(1, 0, 2)         #[sentence_length, batch_size, embedding_size]\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        \n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        #out[seq_len, batch, num_directions * hidden_size]\n",
    "        #h_n, c_n [num_layers * num_directions, batch, hidden_size]\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        #print(out.shape) #20, 16, 32\n",
    "        \n",
    "        #split the output of lstm into forward and backward\n",
    "        #(forward_out, backward_out) = torch.chunk(out, 2, dim = 2)\n",
    "        #out = forward_out + backward_out  #[seq_len, batch, hidden_size]\n",
    "        #out = out.permute(1, 0, 2)  #[batch, seq_len, hidden_size] #16,20,16\n",
    "        \n",
    "        h_n = h_n.permute(1, 0, 2)  #[batch, num_layers * num_directions,  hidden_size]\n",
    "        h_n = torch.sum(h_n, dim=1) #[batch, 1,  hidden_size]\n",
    "        h_n = h_n.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        \n",
    "        #attention_w = self.attention_weights_layer(h_n)  #[batch, hidden_size]\n",
    "        #attention_w = attention_w.unsqueeze(dim=1) #[batch, 1, hidden_size]\n",
    "        \n",
    "        #print(out.transpose(1,2).shape) #16, 16, 20\n",
    "        #attention_context = torch.bmm(attention_w, out.transpose(1, 2))  #[batch, 1, seq_len]\n",
    "        #softmax_w = F.softmax(attention_context, dim=-1)  #[batch, 1, seq_len]\n",
    "        \n",
    "        #x = torch.bmm(softmax_w, out)  #[batch, 1, hidden_size]\n",
    "        #x = x.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        x = self.liner(h_n)\n",
    "        x = self.act_func(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "947c6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    preds_total = []\n",
    "    labels_total = []\n",
    "    for datas, labels in test_loader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(datas)\n",
    "        loss = loss_func(preds, labels)\n",
    "        \n",
    "        loss_val += loss.item() * datas.size(0)\n",
    "        \n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        preds_total.extend(preds.cpu().numpy().tolist())\n",
    "        labels_total.extend(labels.cpu().numpy().tolist())\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader.dataset)\n",
    "    test_acc = corrects / len(test_loader.dataset)\n",
    "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\n",
    "    print(\"precision \", precision_score(labels_total, preds_total, average='macro'))\n",
    "    print(\"recall \", recall_score(labels_total, preds_total, average='macro'))\n",
    "    print(\"f1 \", f1_score(labels_total, preds_total, average='macro'))\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8f8e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader,test_loader, optimizer, loss_func, epochs):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch: \", epoch)\n",
    "        model.train()\n",
    "        loss_val = 0.0\n",
    "        corrects = 0.0\n",
    "        #i = 0\n",
    "        #preds_total = []\n",
    "        #labels_total = []\n",
    "        for datas, labels in train_loader:\n",
    "            #i = i+1\n",
    "            #print(i/len(train_loader)*100, \"%\")\n",
    "            datas = datas.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(datas)\n",
    "            loss = loss_func(preds, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_val += loss.item() * datas.size(0)\n",
    "            \n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            #print(preds)\n",
    "            #print(labels)\n",
    "            #preds_total.extend(preds.cpu().numpy().tolist())\n",
    "            #labels_total.extend(labels.cpu().numpy().tolist())\n",
    "            corrects += torch.sum(preds == labels).item()\n",
    "        train_loss = loss_val / len(train_loader.dataset)\n",
    "        train_acc = corrects / len(train_loader.dataset)\n",
    "        if(epoch % 1== 0):\n",
    "            print(\"Train Loss: {}, Train Acc: {}\".format(train_loss, train_acc))\n",
    "            #print(precision_score(labels_total, preds_total, average='macro'))\n",
    "            #print(recall_score(labels_total, preds_total, average='macro'))\n",
    "            #print(f1_score(labels_total, preds_total, average='macro'))\n",
    "            test_acc = test(model, test_loader, loss_func)\n",
    "            if(best_val_acc < test_acc):\n",
    "                best_val_acc = test_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aed94221",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor()\n",
    "train_datasets, test_datasets = processor.get_datasets(vocab_size=vocab_size, embedding_size=embedding_size, max_len=sentence_max_len)\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce170b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "444ceae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMModel(embedding_size, hidden_size, num_layers, num_directions, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a23bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f375a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMModel(\n",
       "  (lstm): LSTM(100, 16, bidirectional=True)\n",
       "  (liner): Linear(in_features=16, out_features=5, bias=True)\n",
       "  (act_func): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eff44580",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8d14fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51013f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Train Loss: 0.40347099904707134, Train Acc: 0.49309245483528164\n",
      "Test Loss: 0.3253706007921975, Test Acc: 0.6350429884375927\n",
      "precision  0.4706953254317243\n",
      "recall  0.47926499517650034\n",
      "f1  0.45907739445408646\n",
      "epoch:  1\n",
      "Train Loss: 0.2992464621732797, Train Acc: 0.6620154322413714\n",
      "Test Loss: 0.27704943037513735, Test Acc: 0.6919656092499259\n",
      "precision  0.5731887731634194\n",
      "recall  0.5507231240311464\n",
      "f1  0.5267687136783612\n",
      "epoch:  2\n",
      "Train Loss: 0.25287434533641184, Train Acc: 0.7256849789770364\n",
      "Test Loss: 0.27498105935268646, Test Acc: 0.6890008894159502\n",
      "precision  0.5500026719142835\n",
      "recall  0.5780226897014432\n",
      "f1  0.5361988656216854\n",
      "epoch:  3\n",
      "Train Loss: 0.22592063300965295, Train Acc: 0.7590444947558102\n",
      "Test Loss: 0.28274089823375637, Test Acc: 0.68781500148236\n",
      "precision  0.5492891491493117\n",
      "recall  0.614125381010189\n",
      "f1  0.558657327917528\n",
      "epoch:  4\n",
      "Train Loss: 0.2078586101672436, Train Acc: 0.7808991359793005\n",
      "Test Loss: 0.2651351072775515, Test Acc: 0.7103468722205751\n",
      "precision  0.5738128045745909\n",
      "recall  0.6209380362092654\n",
      "f1  0.5725737978435069\n",
      "epoch:  5\n",
      "Train Loss: 0.1936034417221461, Train Acc: 0.7971630550293397\n",
      "Test Loss: 0.2672691905300588, Test Acc: 0.7204269196560925\n",
      "precision  0.5917338823946949\n",
      "recall  0.6317206406722926\n",
      "f1  0.5823912411755824\n",
      "epoch:  6\n",
      "Train Loss: 0.18102615727624197, Train Acc: 0.8125952964006838\n",
      "Test Loss: 0.2697632433203254, Test Acc: 0.7115327601541654\n",
      "precision  0.5691637450304505\n",
      "recall  0.6147651819786413\n",
      "f1  0.5720904791788797\n",
      "epoch:  7\n",
      "Train Loss: 0.17042772361469682, Train Acc: 0.8260407522062561\n",
      "Test Loss: 0.2922716456132157, Test Acc: 0.6978950489178772\n",
      "precision  0.5622054719300521\n",
      "recall  0.627807701590602\n",
      "f1  0.5686753235948897\n",
      "epoch:  8\n",
      "Train Loss: 0.16168960780545621, Train Acc: 0.8368987663447766\n",
      "Test Loss: 0.27748652653182754, Test Acc: 0.7189445597391046\n",
      "precision  0.5767757820008217\n",
      "recall  0.6225322480642642\n",
      "f1  0.5794767450215492\n",
      "epoch:  9\n",
      "Train Loss: 0.15236977745885963, Train Acc: 0.8461858337568728\n",
      "Test Loss: 0.2909652019533106, Test Acc: 0.7162763118885266\n",
      "precision  0.5837475818632712\n",
      "recall  0.6239773556102675\n",
      "f1  0.5820873233129945\n",
      "epoch:  10\n",
      "Train Loss: 0.14599526375952135, Train Acc: 0.852562029293536\n",
      "Test Loss: 0.3264997980068389, Test Acc: 0.6898903053661429\n",
      "precision  0.561618887069848\n",
      "recall  0.6320114084514951\n",
      "f1  0.5672466687461105\n",
      "epoch:  11\n",
      "Train Loss: 0.13932333668438382, Train Acc: 0.8600471284017928\n",
      "Test Loss: 0.2969540677569296, Test Acc: 0.724873999407056\n",
      "precision  0.5803711400623872\n",
      "recall  0.628707396697183\n",
      "f1  0.5915028670916582\n",
      "epoch:  12\n",
      "Train Loss: 0.13487402314631494, Train Acc: 0.8635124520630227\n",
      "Test Loss: 0.3083193658404063, Test Acc: 0.7124221761043581\n",
      "precision  0.5750893135572178\n",
      "recall  0.6161199608218798\n",
      "f1  0.573330210532027\n",
      "epoch:  13\n",
      "Train Loss: 0.1286563467889651, Train Acc: 0.8747401007254078\n",
      "Test Loss: 0.32658714327266514, Test Acc: 0.7058997924696117\n",
      "precision  0.5718723634501336\n",
      "recall  0.6210671246921532\n",
      "f1  0.5720144358884978\n",
      "epoch:  14\n",
      "Train Loss: 0.12372196842786572, Train Acc: 0.8794529409046805\n",
      "Test Loss: 0.3474822072602152, Test Acc: 0.7038244885858287\n",
      "precision  0.5713719063510494\n",
      "recall  0.6325616400572418\n",
      "f1  0.5780130772380749\n",
      "epoch:  15\n",
      "Train Loss: 0.1192448795308568, Train Acc: 0.8834265120362241\n",
      "Test Loss: 0.3540974753637396, Test Acc: 0.6890008894159502\n",
      "precision  0.5595817197400297\n",
      "recall  0.6219242824084811\n",
      "f1  0.5587735160761464\n",
      "epoch:  16\n",
      "Train Loss: 0.1150641597203939, Train Acc: 0.8871228572748695\n",
      "Test Loss: 0.37590271644482015, Test Acc: 0.6845538096649867\n",
      "precision  0.5512225828003193\n",
      "recall  0.6200387958581863\n",
      "f1  0.5606133157430355\n",
      "epoch:  17\n",
      "Train Loss: 0.10989271175922359, Train Acc: 0.8940535045973294\n",
      "Test Loss: 0.35739818404451024, Test Acc: 0.703528016602431\n",
      "precision  0.5604205453884263\n",
      "recall  0.6036318224988129\n",
      "f1  0.5628409345132316\n",
      "epoch:  18\n",
      "Train Loss: 0.10682284654402685, Train Acc: 0.8965023333179319\n",
      "Test Loss: 0.36761751378674623, Test Acc: 0.7017491847020456\n",
      "precision  0.5606832311638624\n",
      "recall  0.6181790456120183\n",
      "f1  0.5683662255147999\n",
      "epoch:  19\n",
      "Train Loss: 0.10280524395281972, Train Acc: 0.9018620339139676\n",
      "Test Loss: 0.3867250923017631, Test Acc: 0.7029350726356359\n",
      "precision  0.5760738020233847\n",
      "recall  0.6106074169028585\n",
      "f1  0.5676425217012677\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, test_loader, optimizer, loss_func, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d76342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2976014317142267, Test Acc: 0.724873999407056\n",
      "precision  0.581392194613668\n",
      "recall  0.625047549188562\n",
      "f1  0.58948431846565\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model, test_loader, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "312fd4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7147939519715387"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb8782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
