{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6817ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d42ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edcf0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def read_text(self,is_train_data):\n",
    "        #read that data\n",
    "        #is_train_data==True   reading the training data \n",
    "        #is_train_data==False  reading the testing data \n",
    "        datas = []\n",
    "        labels = []\n",
    "        if(is_train_data):\n",
    "            #training data folder\n",
    "            INFOCOM_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/train//INFOCOM/\"\n",
    "            ISCAS_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/train//ISCAS/\"\n",
    "            SIGGRAPH_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/train//SIGGRAPH/\"\n",
    "            VLDB_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/train//VLDB/\"\n",
    "            WWW_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/train//WWW/\" \n",
    "        else:\n",
    "            #testing data folder\n",
    "            INFOCOM_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/valid/INFOCOM/\"\n",
    "            ISCAS_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/valid/ISCAS/\"\n",
    "            SIGGRAPH_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/valid/SIGGRAPH/\"\n",
    "            VLDB_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/valid/VLDB/\"\n",
    "            WWW_path = \"C://Users//Jin Xu//Desktop//NLP_project//new_data/\"+str(fold)+\"/valid/WWW/\"\n",
    "        INFOCOM_files= os.listdir(INFOCOM_path)\n",
    "        ISCAS_files = os.listdir(ISCAS_path)\n",
    "        SIGGRAPH_files = os.listdir(SIGGRAPH_path)\n",
    "        VLDB_files = os.listdir(VLDB_path)\n",
    "        WWW_files = os.listdir(WWW_path)\n",
    "        \n",
    "        \n",
    "        for file_name in INFOCOM_files: \n",
    "            file_position = INFOCOM_path + file_name\n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:  \n",
    "                data = f.read()   \n",
    "                datas.append(data)\n",
    "                labels.append([1,0,0,0,0]) #transform the label into vector\n",
    "        \n",
    "        for file_name in ISCAS_files:\n",
    "            file_position = ISCAS_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,1,0,0,0]) #transform the label into vector\n",
    "\n",
    "        for file_name in SIGGRAPH_files:\n",
    "            file_position = SIGGRAPH_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,1,0,0]) #transform the label into vector\n",
    "\n",
    "        for file_name in VLDB_files:\n",
    "            file_position = VLDB_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,0,1,0]) #transform the label into vector\n",
    "\n",
    "        for file_name in WWW_files:\n",
    "            file_position = WWW_path + file_name \n",
    "            with open(file_position, \"r\",encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                datas.append(data)\n",
    "                labels.append([0,0,0,0,1]) #transform the label into vector\n",
    "        return datas, labels\n",
    "    \n",
    "    def word_count(self, datas):\n",
    "        dic = {}\n",
    "        for data in datas:\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower()\n",
    "                if(word in dic):\n",
    "                    dic[word] += 1\n",
    "                else:\n",
    "                    dic[word] = 1\n",
    "        word_count_sorted = sorted(dic.items(), key=lambda item:item[1], reverse=True)\n",
    "        return  word_count_sorted\n",
    "    \n",
    "    def word_index(self, datas, vocab_size):\n",
    "        word_count_sorted = self.word_count(datas)\n",
    "        word2index = {}\n",
    "        word2index[\"<unk>\"] = 0\n",
    "        word2index[\"<pad>\"] = 1\n",
    "        \n",
    "        vocab_size = min(len(word_count_sorted), vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            word = word_count_sorted[i][0]\n",
    "            word2index[word] = i + 2\n",
    "          \n",
    "        return word2index, vocab_size\n",
    "    \n",
    "    def get_datasets(self, vocab_size, embedding_size, max_len):\n",
    "        train_datas, train_labels = self.read_text(is_train_data=True)\n",
    "        word2index, vocab_size = self.word_index(train_datas, vocab_size)\n",
    "        \n",
    "        test_datas, test_labels = self.read_text(is_train_data = False)\n",
    "        \n",
    "        train_features = []\n",
    "        for data in train_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower() \n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"]) \n",
    "                if(len(feature)==max_len): \n",
    "                    break\n",
    "\n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            train_features.append(feature)\n",
    "            \n",
    "        test_features = []\n",
    "        for data in test_datas:\n",
    "            feature = []\n",
    "            data_list = data.split()\n",
    "            for word in data_list:\n",
    "                word = word.lower()\n",
    "                if word in word2index:\n",
    "                    feature.append(word2index[word])\n",
    "                else:\n",
    "                    feature.append(word2index[\"<unk>\"])\n",
    "                if(len(feature)==max_len):\n",
    "                    break\n",
    "            \n",
    "            feature = feature + [word2index[\"<pad>\"]] * (max_len - len(feature))\n",
    "            test_features.append(feature)\n",
    "            \n",
    "        train_features = torch.LongTensor(train_features)\n",
    "        train_labels = torch.FloatTensor(train_labels)\n",
    "        \n",
    "        test_features = torch.LongTensor(test_features)\n",
    "        test_labels = torch.FloatTensor(test_labels)\n",
    "        \n",
    "        embed = nn.Embedding(vocab_size + 2, embedding_size)\n",
    "        train_features = embed(train_features)\n",
    "        test_features = embed(test_features)\n",
    "        \n",
    "        train_features = Variable(train_features, requires_grad=False)\n",
    "        train_datasets = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "        \n",
    "        test_features = Variable(test_features, requires_grad=False)\n",
    "        test_datasets = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "        return train_datasets, test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8437d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "vocab_size = 31000   #size of vocabulary\n",
    "embedding_size = 100   #size of word embedding\n",
    "num_classes = 5    #5 classification\n",
    "sentence_max_len = 20  #the maximum of sentence length\n",
    "hidden_size = 16\n",
    "\n",
    "num_layers = 1  #one layer of lstm\n",
    "num_directions = 2  #bidirectional lstm\n",
    "lr = 1e-2\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66df9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "190331bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-LSTM model\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size,hidden_size, num_layers, num_directions, num_classes):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        \n",
    "        self.input_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        #self.attention_weights_layer = nn.Sequential(\n",
    "         #   nn.Linear(hidden_size, hidden_size),\n",
    "         #   nn.ReLU(inplace=True)\n",
    "        #)\n",
    "        self.liner = nn.Linear(hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #lstm's shape [seq_len, batch, input_size]\n",
    "        #x [batch_size, sentence_length, embedding_size]\n",
    "        x = x.permute(1, 0, 2)         #[sentence_length, batch_size, embedding_size]\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        \n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        #out[seq_len, batch, num_directions * hidden_size]\n",
    "        #h_n, c_n [num_layers * num_directions, batch, hidden_size]\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        #print(out.shape) #20, 16, 32\n",
    "        \n",
    "        #split the output of lstm into forward and backward\n",
    "        #(forward_out, backward_out) = torch.chunk(out, 2, dim = 2)\n",
    "        #out = forward_out + backward_out  #[seq_len, batch, hidden_size]\n",
    "        #out = out.permute(1, 0, 2)  #[batch, seq_len, hidden_size] #16,20,16\n",
    "        \n",
    "        h_n = h_n.permute(1, 0, 2)  #[batch, num_layers * num_directions,  hidden_size]\n",
    "        h_n = torch.sum(h_n, dim=1) #[batch, 1,  hidden_size]\n",
    "        h_n = h_n.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        \n",
    "        #attention_w = self.attention_weights_layer(h_n)  #[batch, hidden_size]\n",
    "        #attention_w = attention_w.unsqueeze(dim=1) #[batch, 1, hidden_size]\n",
    "        \n",
    "        #print(out.transpose(1,2).shape) #16, 16, 20\n",
    "        #attention_context = torch.bmm(attention_w, out.transpose(1, 2))  #[batch, 1, seq_len]\n",
    "        #softmax_w = F.softmax(attention_context, dim=-1)  #[batch, 1, seq_len]\n",
    "        \n",
    "        #x = torch.bmm(softmax_w, out)  #[batch, 1, hidden_size]\n",
    "        #x = x.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        x = self.liner(h_n)\n",
    "        x = self.act_func(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "947c6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    preds_total = []\n",
    "    labels_total = []\n",
    "    for datas, labels in test_loader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(datas)\n",
    "        loss = loss_func(preds, labels)\n",
    "        \n",
    "        loss_val += loss.item() * datas.size(0)\n",
    "        \n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        preds_total.extend(preds.cpu().numpy().tolist())\n",
    "        labels_total.extend(labels.cpu().numpy().tolist())\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader.dataset)\n",
    "    test_acc = corrects / len(test_loader.dataset)\n",
    "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\n",
    "    print(\"precision \", precision_score(labels_total, preds_total, average='macro'))\n",
    "    print(\"recall \", recall_score(labels_total, preds_total, average='macro'))\n",
    "    print(\"f1 \", f1_score(labels_total, preds_total, average='macro'))\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f8e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader,test_loader, optimizer, loss_func, epochs):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch: \", epoch)\n",
    "        model.train()\n",
    "        loss_val = 0.0\n",
    "        corrects = 0.0\n",
    "        #i = 0\n",
    "        #preds_total = []\n",
    "        #labels_total = []\n",
    "        for datas, labels in train_loader:\n",
    "            #i = i+1\n",
    "            #print(i/len(train_loader)*100, \"%\")\n",
    "            datas = datas.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(datas)\n",
    "            loss = loss_func(preds, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_val += loss.item() * datas.size(0)\n",
    "            \n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "            #print(preds)\n",
    "            #print(labels)\n",
    "            #preds_total.extend(preds.cpu().numpy().tolist())\n",
    "            #labels_total.extend(labels.cpu().numpy().tolist())\n",
    "            corrects += torch.sum(preds == labels).item()\n",
    "        train_loss = loss_val / len(train_loader.dataset)\n",
    "        train_acc = corrects / len(train_loader.dataset)\n",
    "        if(epoch % 1== 0):\n",
    "            print(\"Train Loss: {}, Train Acc: {}\".format(train_loss, train_acc))\n",
    "            #print(precision_score(labels_total, preds_total, average='macro'))\n",
    "            #print(recall_score(labels_total, preds_total, average='macro'))\n",
    "            #print(f1_score(labels_total, preds_total, average='macro'))\n",
    "            test_acc = test(model, test_loader, loss_func)\n",
    "            if(best_val_acc < test_acc):\n",
    "                best_val_acc = test_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aed94221",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor()\n",
    "train_datasets, test_datasets = processor.get_datasets(vocab_size=vocab_size, embedding_size=embedding_size, max_len=sentence_max_len)\n",
    "train_loader = torch.utils.data.DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce170b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "444ceae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMModel(embedding_size, hidden_size, num_layers, num_directions, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a23bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f375a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMModel(\n",
       "  (lstm): LSTM(100, 16, bidirectional=True)\n",
       "  (liner): Linear(in_features=16, out_features=5, bias=True)\n",
       "  (act_func): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eff44580",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8d14fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51013f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Train Loss: 0.3898983454775878, Train Acc: 0.5164289426575042\n",
      "Test Loss: 0.318458539878556, Test Acc: 0.6345353675450763\n",
      "precision  0.6168327169968109\n",
      "recall  0.5585706888763164\n",
      "f1  0.5720240378496418\n",
      "epoch:  1\n",
      "Train Loss: 0.29348401858926804, Train Acc: 0.6681873303690016\n",
      "Test Loss: 0.2950113188514983, Test Acc: 0.6659731853906611\n",
      "precision  0.6371308673580053\n",
      "recall  0.6264449240187144\n",
      "f1  0.627745401570764\n",
      "epoch:  2\n",
      "Train Loss: 0.25705598463649776, Train Acc: 0.7210833285211065\n",
      "Test Loss: 0.2837427711175428, Test Acc: 0.683541377716135\n",
      "precision  0.6518431592899901\n",
      "recall  0.6285487554887929\n",
      "f1  0.6320194757482333\n",
      "epoch:  3\n",
      "Train Loss: 0.23839452048626922, Train Acc: 0.7426806028757867\n",
      "Test Loss: 0.288083141854776, Test Acc: 0.6807674526121128\n",
      "precision  0.6515356312392264\n",
      "recall  0.6453121437503517\n",
      "f1  0.6437710195268849\n",
      "epoch:  4\n",
      "Train Loss: 0.2267811220409675, Train Acc: 0.7575792573771438\n",
      "Test Loss: 0.28802723714834666, Test Acc: 0.6786870087840962\n",
      "precision  0.652355378188437\n",
      "recall  0.6460509604088447\n",
      "f1  0.6440787248153572\n",
      "epoch:  5\n",
      "Train Loss: 0.2128821819867508, Train Acc: 0.774672287347693\n",
      "Test Loss: 0.2796414164594627, Test Acc: 0.7029588534442903\n",
      "precision  0.6713664197954714\n",
      "recall  0.6642106578475412\n",
      "f1  0.6651592400210878\n",
      "epoch:  6\n",
      "Train Loss: 0.2061537250531369, Train Acc: 0.7826990818271063\n",
      "Test Loss: 0.2806550999565142, Test Acc: 0.7015718908922792\n",
      "precision  0.6694145017594251\n",
      "recall  0.660535197672728\n",
      "f1  0.6619292140341153\n",
      "epoch:  7\n",
      "Train Loss: 0.19835199920621172, Train Acc: 0.7897441820176705\n",
      "Test Loss: 0.2767299206387484, Test Acc: 0.7059639389736477\n",
      "precision  0.6713626707080371\n",
      "recall  0.6704719205474017\n",
      "f1  0.6701905945394936\n",
      "epoch:  8\n",
      "Train Loss: 0.1935851297992519, Train Acc: 0.7969625223768552\n",
      "Test Loss: 0.27989440738906807, Test Acc: 0.7055016181229773\n",
      "precision  0.6717435202174366\n",
      "recall  0.6663039606289707\n",
      "f1  0.6683357356841478\n",
      "epoch:  9\n",
      "Train Loss: 0.18682917671242302, Train Acc: 0.8066062250967257\n",
      "Test Loss: 0.2846554456983955, Test Acc: 0.7008784096162737\n",
      "precision  0.6628705324422566\n",
      "recall  0.6597709009295359\n",
      "f1  0.6606984305361312\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, test_loader, optimizer, loss_func, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d76342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2769093874891875, Test Acc: 0.7064262598243181\n",
      "precision  0.6722409318861049\n",
      "recall  0.6713702015887902\n",
      "f1  0.67107096378551\n"
     ]
    }
   ],
   "source": [
    "accuracy = test(model, test_loader, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "312fd4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.698797965788257"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb8782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
